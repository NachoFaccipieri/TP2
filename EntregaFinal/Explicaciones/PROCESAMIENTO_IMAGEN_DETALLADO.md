# ğŸ“· PROCESAMIENTO DE IMAGEN - FaceID Completo

## Resumen Ejecutivo

```
ENTRADA: Imagen capturada con cÃ¡mara (RGB, cualquier tamaÃ±o)
   â†“
MTCNN: Detecta rostro en la imagen
   â†“
PIL: Extrae y redimensiona rostro a 160x160
   â†“
FaceNet: Convierte rostro en "huella dactilar facial" (128 nÃºmeros)
   â†“
NumPy: Normaliza la huella (L2 normalization)
   â†“
SALIDA: Vector de 128 nÃºmeros entre -1 y 1 (embedding)
```

---

## ğŸ“¸ PASO 1: Captura de Imagen (OpenCV)

### FunciÃ³n: `capture_frame()`

```python
def capture_frame(camera_index=0, save_last=True):
    """Captura un frame de la cÃ¡mara"""
    global last_captured_image
    
    # Abre la cÃ¡mara
    cap = cv2.VideoCapture(camera_index)
    #   â”œâ”€ camera_index=0 â†’ Primera cÃ¡mara USB/Raspberry Pi Camera
    #   â””â”€ Retorna objeto VideoCapture
    
    if not cap.isOpened():
        return None, 'No se pudo abrir la cÃ¡mara'
    
    # Captura UN frame
    ret, frame = cap.read()
    #   â”œâ”€ ret = True si fue exitoso
    #   â””â”€ frame = numpy array BGR (altura, ancho, 3)
    #              â”œâ”€ OpenCV usa BGR, no RGB
    #              â”œâ”€ Shape: (480, 640, 3) tÃ­picamente
    #              â””â”€ dtype: uint8 (0-255 por canal)
    
    cap.release()
    # Cierra la cÃ¡mara para liberar recursos
    
    if not ret:
        return None, 'No se pudo capturar el frame'
    
    # Guarda una copia para mostrar despuÃ©s
    if save_last:
        with last_image_lock:
            last_captured_image = frame.copy()
            # Se guarda porque /api/camera/last la usa
    
    # Convierte BGR â†’ RGB (OpenCV usa BGR, PIL usa RGB)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    #   â”œâ”€ OpenCV: BGR (Blue, Green, Red)
    #   â”œâ”€ PIL/RGB: RGB (Red, Green, Blue)
    #   â””â”€ Los canales estÃ¡n invertidos, hay que corregir
    
    # Convierte array NumPy a imagen PIL
    img = Image.fromarray(frame_rgb)
    #   â”œâ”€ PIL es mÃ¡s fÃ¡cil de manipular
    #   â””â”€ Se necesita para redimensionamiento
    
    return img, None
```

### Ejemplo Visual

```
CÃ¡mara USB/Raspberry Pi
         â†“
OpenCV abre VideoCapture
         â†“
Captura 1 frame: numpy array
  Shape: (480, 640, 3)
  dtype: uint8
  Valores: 0-255 por pixel por canal
         â†“
Convierte BGR â†’ RGB
  B: [255, 100, 50, ...]
  G: [100, 200, 150, ...]
  R: [50, 255, 100, ...]
         â†“
Crea PIL.Image
  MÃ¡s fÃ¡cil de manipular
         â†“
Retorna PIL.Image object
```

---

## ğŸ” PASO 2: DetecciÃ³n de Rostro (MTCNN)

### FunciÃ³n: `generarEmbedding()` - Parte 1

```python
def generarEmbedding(img):
    """
    ENTRADA: PIL.Image (cualquier tamaÃ±o)
    SALIDA: numpy array de 128 nÃºmeros (embedding)
    """
    
    # Validar que sea RGB
    if img.mode != 'RGB':
        img = img.convert('RGB')
    
    # Convierte PIL.Image a numpy array
    img_array = np.asarray(img)
    #   â”œâ”€ Shape: (altura, ancho, 3)
    #   â”œâ”€ dtype: uint8
    #   â””â”€ Valores: 0-255
    
    # ========== MTCNN: DETECTA ROSTROS ==========
    detections = detector.detect_faces(img_array)
    #   â”œâ”€ detector = MTCNN() (importado al inicio)
    #   â”œâ”€ Detecta TODOS los rostros en la imagen
    #   â”œâ”€ Retorna lista de diccionarios
    #   â””â”€ Ejemplo:
    #      [
    #        {
    #          'box': [x, y, ancho, alto],
    #          'confidence': 0.98,
    #          'keypoints': {'left_eye': [...], ...}
    #        }
    #      ]
    
    if len(detections) == 0:
        return None  # No detectÃ³ ningÃºn rostro
    
    # Toma el PRIMER rostro detectado (el mÃ¡s grande/confiado)
    x, y, w, h = detections[0]['box']
    #   â”œâ”€ x, y = posiciÃ³n superior-izquierda del rostro
    #   â”œâ”€ w = ancho del rostro
    #   â”œâ”€ h = alto del rostro
    #   â””â”€ Pueden ser negativos en los bordes, por eso:
    
    x, y = abs(x), abs(y)  # Asegurar que no sean negativos
    
    # Extrae la regiÃ³n de interÃ©s (ROI) del rostro
    face = img_array[y:y+h, x:x+w]
    #   â”œâ”€ numpy slicing: [fila_inicio:fila_fin, col_inicio:col_fin]
    #   â”œâ”€ Extrae solo el Ã¡rea del rostro
    #   â””â”€ Shape: (h, w, 3)
```

### VisualizaciÃ³n MTCNN

```
Imagen original (640x480)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                               â”‚
â”‚     ğŸ‘¤  â† Rostro detectado                   â”‚
â”‚   (MTCNN dibuja un recuadro aquÃ­)            â”‚
â”‚   â”œâ”€ x=100, y=50                            â”‚
â”‚   â”œâ”€ w=150, h=180                           â”‚
â”‚                                               â”‚
â”‚                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MTCNN retorna:
  'box': [100, 50, 150, 180]
  
Se extrae:
  face = img[50:230, 100:250]  â† Solo el rostro
```

---

## ğŸ“ PASO 3: Redimensionamiento (PIL)

### FunciÃ³n: `generarEmbedding()` - Parte 2

```python
def generarEmbedding(img):
    # ... (cÃ³digo anterior)
    
    # face = array de cualquier tamaÃ±o
    # Ejemplo: (180, 150, 3) si el rostro detectado tiene eso
    
    # ========== REDIMENSIONA A 160x160 ==========
    # FaceNet REQUIERE exactamente 160x160
    
    face = Image.fromarray(face).resize((160, 160))
    #   â”œâ”€ Convierte numpy array â†’ PIL.Image
    #   â”œâ”€ .resize((160, 160)) redimensiona
    #   â”œâ”€ Nueva shape: (160, 160, 3)
    #   â””â”€ IMPORTANTE: Distorsiona si la relaciÃ³n aspecto no es 1:1
    
    # Convierte de vuelta a numpy array
    face = np.asarray(face)
    #   â”œâ”€ Shape: (160, 160, 3)
    #   â”œâ”€ dtype: uint8
    #   â””â”€ Valores: 0-255
    
    # ========== PREPARAR PARA FACENET ==========
    # FaceNet espera un batch (lote), aunque sea de 1 imagen
    
    face = np.expand_dims(face, axis=0)
    #   â”œâ”€ Agrega dimensiÃ³n batch
    #   â”œâ”€ Antes: (160, 160, 3)
    #   â”œâ”€ DespuÃ©s: (1, 160, 160, 3)  â† Batch de 1
    #   â””â”€ Ahora FaceNet puede procesar
```

### VisualizaciÃ³n del Redimensionamiento

```
Rostro extraÃ­do: (180, 150, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ‘¤                    â”‚  â† Rectangular
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

.resize((160, 160)) aplica escalado bilineal
         â†“
Rostro redimensionado: (160, 160, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ğŸ‘¤                     â”‚  â† Cuadrado
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

np.expand_dims(axis=0)
         â†“
Batch: (1, 160, 160, 3)
  [
    Imagen 1: (160, 160, 3)
  ]
```

---

## ğŸ§  PASO 4: GeneraciÃ³n de Embedding (FaceNet)

### FunciÃ³n: `generarEmbedding()` - Parte 3

```python
def generarEmbedding(img):
    # ... (cÃ³digo anterior)
    
    # face = (1, 160, 160, 3) - Batch de 1 imagen
    
    # ========== FACENET: GENERA EMBEDDING ==========
    print("Generando embedding...")
    
    embedding = embedder.embeddings(face)[0]
    #   â”œâ”€ embedder = FaceNet() (importado al inicio)
    #   â”œâ”€ FaceNet es una red neuronal convolucional
    #   â”œâ”€ embedder.embeddings(face) retorna:
    #   â”‚  â”œâ”€ Array shape: (1, 128) si batch_size=1
    #   â”‚  â””â”€ Cada elemento = un vector de 128 nÃºmeros
    #   â”œâ”€ [0] â†’ Toma el primer (Ãºnico) embedding
    #   â””â”€ embedding = array de 128 nÃºmeros
    
    # embedding = [0.123, -0.456, 0.789, ..., -0.234]  â† 128 nÃºmeros
```

### Â¿QuÃ© es FaceNet?

```
FaceNet es una red neuronal entrenada para:
1. Tomar una imagen de rostro (160x160x3)
2. Extraer caracterÃ­sticas faciales
3. Convertirlas en un vector de 128 nÃºmeros

Este vector se llama EMBEDDING o "huella dactilar facial"

Propiedades importantes:
â”œâ”€ Rostros similares â†’ embeddings similares
â”œâ”€ Rostros diferentes â†’ embeddings diferentes
â”œâ”€ Invariante a:
â”‚  â”œâ”€ Pose (Ã¡ngulo de la cabeza)
â”‚  â”œâ”€ IluminaciÃ³n
â”‚  â”œâ”€ ExpresiÃ³n facial
â”‚  â””â”€ Edad (dentro de lÃ­mites)
â”œâ”€ Resultado: 128 nÃºmeros float32
â””â”€ Normalmente entre -1 y 1 (despuÃ©s de normalizar)
```

### Arquitectura Simplificada

```
INPUT: (160, 160, 3)
   â†“
Conv2D(64, 3x3) â†’ ReLU â†’ BatchNorm
   â†“
Conv2D(64, 3x3) â†’ ReLU â†’ BatchNorm
   â†“
MaxPool(2x2)  [ahora 80x80x64]
   â†“
Conv2D(128, 3x3) â†’ ReLU â†’ BatchNorm
   â†“
[... mÃ¡s capas convolucionales ...]
   â†“
GlobalAveragePooling  [flatten]
   â†“
Dense(128)  â† AQUI se genera el embedding
   â†“
Dense(512)  â† Layer interno adicional
   â†“
OUTPUT: 128 nÃºmeros (embedding)
```

---

## ğŸ“Š PASO 5: NormalizaciÃ³n L2 (NumPy)

### FunciÃ³n: `generarEmbedding()` - Parte 4

```python
def generarEmbedding(img):
    # ... (cÃ³digo anterior)
    
    embedding = embedder.embeddings(face)[0]
    # embedding = [0.123, -0.456, 0.789, ..., -0.234]
    
    # ========== NORMALIZACIÃ“N L2 ==========
    # L2 norm: ||v|| = sqrt(v1Â² + v2Â² + ... + v128Â²)
    
    norm = np.linalg.norm(embedding)
    #   â”œâ”€ np.linalg.norm() calcula la norma L2
    #   â”œâ”€ Ejemplo numÃ©rico:
    #   â”‚  embedding = [3, 4]
    #   â”‚  norm = sqrt(3Â² + 4Â²) = sqrt(25) = 5
    #   â”‚
    #   â””â”€ Para embedding real: norm â‰ˆ 1.5-2.5 tÃ­picamente
    
    if norm > 0:
        embedding = embedding / norm
        #   â”œâ”€ Divide cada elemento por la norma
        #   â”œâ”€ Ejemplo:
        #   â”‚  embedding = [3/5, 4/5] = [0.6, 0.8]
        #   â”‚
        #   â””â”€ Ahora ||embedding|| = 1 exactamente
    
    return embedding
    # Retorna vector unitario: magnitud = 1
```

### Â¿Por quÃ© normalizar?

```
SIN normalizaciÃ³n:
â”œâ”€ embedding1 = [0.5, 0.3, 0.7, ...]  â†’ norm â‰ˆ 2.1
â”œâ”€ embedding2 = [0.5, 0.3, 0.7, ...]  â†’ norm â‰ˆ 2.1
â””â”€ Â¿Son iguales? SÃ­, pero...
   El modelo podrÃ­a aprender a generar
   embeddings de diferentes magnitudes

CON normalizaciÃ³n L2:
â”œâ”€ embedding1 = [0.238, 0.143, 0.333, ...]  â†’ norm = 1.0
â”œâ”€ embedding2 = [0.238, 0.143, 0.333, ...]  â†’ norm = 1.0
â””â”€ Â¿Son iguales? SÃ­, y ademÃ¡s:
   Todos los embeddings tienen magnitud 1
   Ahora solo importa la DIRECCIÃ“N, no la magnitud
   
VENTAJA: Distancia entre embeddings mÃ¡s consistente
```

### Ejemplo NumÃ©rico Completo

```
Embedding RAW de FaceNet:
embedding = [0.250, -0.180, 0.340, ..., -0.120]  â† 128 nÃºmeros
(valores varÃ­an, tÃ­picamente entre -1 y 1)

CÃ¡lculo de norma L2:
norm = sqrt(0.250Â² + (-0.180)Â² + 0.340Â² + ... + (-0.120)Â²)
     = sqrt(0.0625 + 0.0324 + 0.1156 + ... + 0.0144)
     = sqrt(aproximadamente 1.8)
     â‰ˆ 1.34

NormalizaciÃ³n L2:
embedding_normalizado = embedding / 1.34
                      = [0.250/1.34, -0.180/1.34, 0.340/1.34, ..., -0.120/1.34]
                      = [0.187, -0.134, 0.254, ..., -0.090]

VerificaciÃ³n:
norm_nuevo = sqrt(0.187Â² + (-0.134)Â² + 0.254Â² + ... + (-0.090)Â²)
           = sqrt(0.0349 + 0.0180 + 0.0645 + ... + 0.0081)
           = sqrt(exactamente 1.0)
           = 1.0 âœ“
```

---

## ğŸ”„ Flujo Completo Visual

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  PROCESAMIENTO COMPLETO DE IMAGEN              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ENTRADA
   â”‚
   â”œâ”€ Formato: Imagen fÃ­sica (640x480 aprox)
   â”œâ”€ Source: CÃ¡mara USB / Raspberry Pi Camera
   â””â”€ Objetivo: Obtener embedding para comparar

PASO 1: OpenCV - capture_frame()
   â”‚
   â”œâ”€ cv2.VideoCapture(0)
   â”œâ”€ cv2.cvtColor(BGR â†’ RGB)
   â””â”€ Image.fromarray()
   â”‚
   â””â”€> PIL.Image (cualquier tamaÃ±o, RGB)

PASO 2: MTCNN - Detector de Rostros
   â”‚
   â”œâ”€ detector.detect_faces(img_array)
   â”œâ”€ Busca TODOS los rostros
   â”œâ”€ Retorna: caja delimitadora (x, y, w, h)
   â””â”€ Extrae regiÃ³n del rostro
   â”‚
   â””â”€> numpy array (h, w, 3) - Solo rostro

PASO 3: PIL - Redimensionamiento
   â”‚
   â”œâ”€ Image.fromarray(face)
   â”œâ”€ .resize((160, 160))
   â”œâ”€ np.asarray()
   â”œâ”€ np.expand_dims() â†’ Batch de 1
   â””â”€ (1, 160, 160, 3)
   â”‚
   â””â”€> numpy array (1, 160, 160, 3) - Listo para FaceNet

PASO 4: FaceNet - Generador de Embeddings
   â”‚
   â”œâ”€ embedder.embeddings(face)
   â”œâ”€ Red neuronal convolucional
   â”œâ”€ 14+ capas entrenadas
   â”œâ”€ Extrae caracterÃ­sticas faciales
   â””â”€ Retorna: (1, 128)
   â”‚
   â”œâ”€> embedding[0] = array de 128 nÃºmeros
   â””â”€> Valores: tÃ­picamente entre -1 y 1

PASO 5: NumPy - NormalizaciÃ³n L2
   â”‚
   â”œâ”€ np.linalg.norm(embedding)
   â”œâ”€ Calcula ||v|| = sqrt(suma de cuadrados)
   â”œâ”€ embedding = embedding / norm
   â””â”€ Resultado: vector unitario (magnitud = 1)
   â”‚
   â””â”€> embedding normalizado (128 nÃºmeros, ||v|| = 1)

SALIDA
   â”‚
   â””â”€ array de 128 nÃºmeros float32, normalizados L2
      Listo para guardar o comparar con otros embeddings
```

---

## ğŸ’¾ Flujo en el CÃ³digo Real

```python
def iniciar_registro():
    # Paso 1: Captura
    img, err = capture_frame()
    # â†’ retorna PIL.Image
    
    # Paso 2-5: Procesa
    embedding = generarEmbedding(img)
    # â†’ retorna array(128,) normalizado
    
    # Guarda
    save_embedding(embedding, "Nacho")
    # â†’ guarda en embeddings.txt
```

---

## ğŸ“Š Tabla Resumen

| Paso | FunciÃ³n | Input | Processing | Output |
|------|---------|-------|-----------|--------|
| 1 | `capture_frame()` | CÃ¡mara USB | OpenCV VideoCapture | PIL.Image (RGB) |
| 2 | `generarEmbedding()` (MTCNN) | PIL.Image | DetecciÃ³n de rostro | array(h,w,3) |
| 3 | `generarEmbedding()` (PIL) | array(h,w,3) | Redimensiona 160x160 | array(1,160,160,3) |
| 4 | `generarEmbedding()` (FaceNet) | array(1,160,160,3) | Red neuronal | array(1,128) |
| 5 | `generarEmbedding()` (NumPy) | array(128) | NormalizaciÃ³n L2 | array(128) normalizado |

---

## ğŸ” Ejemplo Paso a Paso Real

```
Usuario presiona botÃ³n â†’ iniciar_registro() se ejecuta

1ï¸âƒ£ capture_frame():
   - Abre /dev/video0 (cÃ¡mara)
   - Lee 1 frame: BGR (480, 640, 3) uint8
   - Convierte a RGB
   - Retorna: PIL.Image objeto

2ï¸âƒ£ generarEmbedding() - MTCNN:
   - Detecta rostro en posiciÃ³n (100, 80, 200, 240)
   - Extrae regiÃ³n: img_array[80:320, 100:300]
   - Resultado: array (240, 200, 3)

3ï¸âƒ£ generarEmbedding() - PIL:
   - Convierte a PIL.Image
   - Redimensiona a (160, 160)
   - Expande dims: (1, 160, 160, 3)

4ï¸âƒ£ generarEmbedding() - FaceNet:
   - Pasa por 14 capas convolucionales
   - Extrae caracterÃ­sticas
   - Retorna: array (1, 128)
     ejemplo: [0.23, -0.15, 0.42, ..., -0.08]
   - Toma [0]: array (128,)

5ï¸âƒ£ generarEmbedding() - NumPy L2:
   - norm = sqrt(suma de (0.23Â² + (-0.15)Â² + ... + (-0.08)Â²))
          = 1.56
   - embedding = embedding / 1.56
   - Resultado: [0.147, -0.096, 0.269, ..., -0.051]
   - Nueva norm = 1.0 âœ“

6ï¸âƒ£ save_embedding():
   - Guarda en embeddings.txt:
     [0.147, -0.096, 0.269, ..., -0.051]
   - Guarda en names.txt:
     Nacho

âœ… Registro completado
```

---

## ğŸ“ Puntos Clave

1. **OpenCV captura**: Imagen RGB estÃ¡ndar
2. **MTCNN detecta**: DÃ³nde estÃ¡ el rostro
3. **PIL redimensiona**: A tamaÃ±o fijo (160x160)
4. **FaceNet convierte**: A "huella dactilar" (128 nÃºmeros)
5. **NumPy normaliza**: Para comparaciÃ³n consistente
6. **Resultado**: Vector de 128 nÃºmeros que representa caracterÃ­sticas faciales Ãºnicas

---

## ğŸš€ Uso Posterior (Reconocimiento)

Cuando alguien timbra:

```python
# Procesa imagen igual que en registro
new_embedding = generarEmbedding(img)  # array(128,) normalizado

# Carga embeddings guardados
stored_embeddings, names = load_embeddings()

# Calcula distancias
distancias = [np.linalg.norm(new_embedding - emb) for emb in stored_embeddings]

# Encuentra similitud mÃ¡s cercana
min_dist = min(distancias)

if min_dist < 0.8:  # Si distancia < umbral
    print(f"Coincidencia: {names[idx]}")
else:
    print("No coincide")
```

**Â¿Por quÃ© funciona?**
- Embeddings normalizados de rostros similares â†’ distancia pequeÃ±a
- Embeddings de rostros diferentes â†’ distancia grande

